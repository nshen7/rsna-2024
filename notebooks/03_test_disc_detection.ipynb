{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8247669c-586a-46d1-953b-63501cb71556",
   "metadata": {},
   "source": [
    "# Contents\n",
    "<div style=\"font-size: 16px;\">\n",
    "    This notebook evaluates how trained disc detection models perform on test sets, with respect to each spinal diagnosed condition. We take the model trained from the 'best epoch' that resulted in highest validation precision/recall.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022343c7-fd76-461f-9184-ea83b6f4b1da",
   "metadata": {},
   "source": [
    "# Install libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086790b0-3358-4355-85a9-86121e560bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pydicom -q\n",
    "!pip install torch==2.1 torchvision==0.16 -q\n",
    "!pip install -qU pycocotools\n",
    "!pip install -qU wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12116a-57a8-4caf-9107-7685e2b27a5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659bf7f6-b2fa-4e1a-8e3f-2ff17e8ee866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in list(globals()):\n",
    "    if not name.startswith(\"_\"):  # Avoid deleting built-in and special variables\n",
    "        del globals()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dec123b-44aa-43eb-bf2d-9a5030fcda33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d765d92-7c6a-44ee-88c5-9b39a42d4d80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a629c8-b2bc-458d-af8c-8a9deee56dc0",
   "metadata": {},
   "source": [
    "## Install and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9888de73-400a-4302-8d68-b6fe1514037c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6face8-8c80-4790-b234-fcc9a0171411",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b38286ce-9001-4e1e-851f-b4ac76cf61e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "PROJECT_DIR = '/home/jupyter'\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "SRC_DIR = os.path.join(PROJECT_DIR, 'src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459e879-6fde-492a-832a-54228d2dee85",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc49c84d-2eac-448a-9f2d-fbb75865a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SRC_DIR, 'pipeline_disc_detection.py')) as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b83dad-94d1-4332-8c41-4de34aa309e9",
   "metadata": {},
   "source": [
    "# Evaluation on test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3b2def-2893-4063-9ed8-419513f40e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_on_test_set(condition, epoch, config):\n",
    "    \n",
    "    MODEL_DIR = os.path.join(PROJECT_DIR, 'models', '02_train_disc_detection', condition)\n",
    "\n",
    "    # Read in metadata\n",
    "    test_df = pd.read_csv(os.path.join(DATA_DIR, 'processed_metadata', condition, 'test.csv'))\n",
    "    \n",
    "    # Create data loder\n",
    "    dataset_test = RSNAMultipleBBoxesDataset(test_df, w = config['box_w'], h_l1_l4 = config['box_h_l1_l4'], h_l5 = config['box_h_l5'])\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=utils.collate_fn,\n",
    "        num_workers=os.cpu_count()\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    trained_model = load_model_disc_detection(state_dict=torch.load(os.path.join(f\"{MODEL_DIR}/epoch_{epoch}/model_dict.pt\"))).to(device)\n",
    "    \n",
    "    # Evaluate on test set (this evaluate function is from https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py)\n",
    "    evaluate(trained_model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ec0f8-689e-45ce-a24b-4e042120de16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spinal Canal Stenosis (captured by Sagittal T2/STIR images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882cefd8-4c18-4076-94f2-001646d8dd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/38]  eta: 0:02:02  model_time: 2.4348 (2.4348)  evaluator_time: 0.0258 (0.0258)  time: 3.2106  data: 0.7474  max mem: 5491\n",
      "Test:  [37/38]  eta: 0:00:01  model_time: 1.1387 (1.1582)  evaluator_time: 0.0244 (0.0250)  time: 1.1959  data: 0.0271  max mem: 5499\n",
      "Test: Total time: 0:00:46 (1.2321 s / it)\n",
      "Averaged stats: model_time: 1.1387 (1.1582)  evaluator_time: 0.0244 (0.0250)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.23s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.764\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.942\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.764\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.846\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.846\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.846\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "CONDITION = 'SpinalCanalStenosis'\n",
    "BEST_EPOCH = 1\n",
    "\n",
    "CONFIG = dict(\n",
    "    num_epochs=3,\n",
    "    batch_size=10,\n",
    "    lr=0.0001,\n",
    "    lr_step_size=3,\n",
    "    lr_gamma=0.1,\n",
    "    box_w = 70, # width of the bounding boxes\n",
    "    box_h_l1_l4 = 30, # height of the boxes for levels from L1/L2 to L4/L5\n",
    "    box_h_l5 = 40 # width of the boxes for level L5/S1\n",
    ")\n",
    "\n",
    "evaluate_on_test_set(condition = CONDITION, epoch = BEST_EPOCH, config = CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a1df6-e641-4ed2-aa52-4630a93a76d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Left Neural Foraminal Narrowing (captured by Sagittal T1 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f04cb359-6e32-461c-8567-4197c52759ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/62]  eta: 0:01:57  model_time: 1.2168 (1.2168)  evaluator_time: 0.0272 (0.0272)  time: 1.9001  data: 0.6537  max mem: 5499\n",
      "Test:  [61/62]  eta: 0:00:01  model_time: 1.3449 (1.2649)  evaluator_time: 0.0256 (0.0307)  time: 1.3978  data: 0.0266  max mem: 5499\n",
      "Test: Total time: 0:01:23 (1.3389 s / it)\n",
      "Averaged stats: model_time: 1.3449 (1.2649)  evaluator_time: 0.0256 (0.0307)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.35s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.695\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.943\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.862\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.695\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.732\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.794\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.794\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.794\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "CONDITION = 'LeftNeuralForaminalNarrowing'\n",
    "BEST_EPOCH = 1\n",
    "\n",
    "CONFIG = dict(\n",
    "    num_epochs=3,\n",
    "    batch_size=10,\n",
    "    lr=0.0001,\n",
    "    lr_step_size=3,\n",
    "    lr_gamma=0.1,\n",
    "    box_w = 70, # width of the bounding boxes\n",
    "    box_h_l1_l4 = 30, # height of the boxes for levels from L1/L2 to L4/L5\n",
    "    box_h_l5 = 40 # width of the boxes for level L5/S1\n",
    ")\n",
    "\n",
    "evaluate_on_test_set(condition = CONDITION, epoch = BEST_EPOCH, config = CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774d58f-80ea-4ac5-a1b8-0287f86fd61a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Right Neural Foraminal Narrowing (captured by Sagittal T1 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14be6e20-215e-4d0b-9ef2-40483ea79e96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/62]  eta: 0:02:10  model_time: 1.3058 (1.3058)  evaluator_time: 0.0277 (0.0277)  time: 2.0987  data: 0.7626  max mem: 5499\n",
      "Test:  [61/62]  eta: 0:00:01  model_time: 1.3005 (1.3117)  evaluator_time: 0.0255 (0.0302)  time: 1.3269  data: 0.0260  max mem: 5499\n",
      "Test: Total time: 0:01:25 (1.3842 s / it)\n",
      "Averaged stats: model_time: 1.3005 (1.3117)  evaluator_time: 0.0255 (0.0302)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.35s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.721\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.963\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.721\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.770\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "CONDITION = 'RightNeuralForaminalNarrowing'\n",
    "BEST_EPOCH = 1\n",
    "\n",
    "CONFIG = dict(\n",
    "    num_epochs=3,\n",
    "    batch_size=10,\n",
    "    lr=0.0001,\n",
    "    lr_step_size=3,\n",
    "    lr_gamma=0.1,\n",
    "    box_w = 70, # width of the bounding boxes\n",
    "    box_h_l1_l4 = 30, # height of the boxes for levels from L1/L2 to L4/L5\n",
    "    box_h_l5 = 40 # width of the boxes for level L5/S1\n",
    ")\n",
    "\n",
    "evaluate_on_test_set(condition = CONDITION, epoch = BEST_EPOCH, config = CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b025c5-78be-4d2c-81fd-c6553e0421cc",
   "metadata": {},
   "source": [
    "# Crop predicted boxes for testing severity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01132f33-1c3b-4b7d-953f-13ebafc7846b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABELS_DICT = {\n",
    "    1: \"L1_L2\",\n",
    "    2: \"L2_L3\",\n",
    "    3: \"L3_L4\",\n",
    "    4: \"L4_L5\",\n",
    "    5: \"L5_S1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8104765c-b549-46ac-be24-6eec18b1bb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_boxes(pred):\n",
    "    best_boxes = {}\n",
    "\n",
    "    for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):\n",
    "        if label.item() not in best_boxes or score > best_boxes[label.item()]['score']:\n",
    "            best_boxes[label.item()] = {'box': box.tolist(), 'score': score.item()}\n",
    "\n",
    "    result = {\n",
    "        'boxes': [entry['box'] for entry in best_boxes.values()],\n",
    "        'labels': list(best_boxes.keys()),\n",
    "        'scores': [entry['score'] for entry in best_boxes.values()]\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def crop_bbox(image, bbox):\n",
    "    x0, y0, x1, y1 = bbox\n",
    "\n",
    "    cropped_img = torchvision.transforms.functional.crop(\n",
    "        image,\n",
    "        top=round(int(y0)),\n",
    "        left=round(int(x0)),\n",
    "        height=round(int(y1 - y0)),\n",
    "        width=round(int(x1 - x0))\n",
    "    )\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "def plot_crop(image, bboxes):\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=1, figsize=(4,3))\n",
    "    plt.subplots_adjust(top=2)\n",
    "\n",
    "    for i in range(len(bboxes['boxes'])):\n",
    "        label_i = bboxes['labels'][i] - 1\n",
    "        label = LABELS_DICT[label_i + 1]\n",
    "        score = bboxes['scores'][i]\n",
    "        bbox = bboxes['boxes'][i]\n",
    "\n",
    "        cropped_img = crop_bbox(image, bbox)\n",
    "        cropped_img = cropped_img[0, :]\n",
    "\n",
    "        ax[label_i].set_axis_off()\n",
    "        ax[label_i].imshow(cropped_img, cmap=\"bone\")\n",
    "        ax[label_i].set_title(f\"{label} ({'{:.2f}'.format(score)})\")\n",
    "        \n",
    "\n",
    "def save_crop(image, bboxes, target, crop_dir):\n",
    "    series_id = target['series_id']\n",
    "    study_id = target['study_id']\n",
    "    instance_number = target['instance_number']\n",
    "\n",
    "    for i in range(len(bboxes['boxes'])):\n",
    "        label = LABELS_DICT[bboxes['labels'][i]]\n",
    "\n",
    "        dirname = f'{crop_dir}/{study_id}/{series_id}/{label}'\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "        filepath = os.path.join(dirname, f'{instance_number}.pt')\n",
    "\n",
    "        bbox = bboxes['boxes'][i]\n",
    "\n",
    "        cropped_img = crop_bbox(image, bbox)\n",
    "        torch.save(cropped_img, filepath)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c8d582-6e11-4245-852e-e83ed4a7ee21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_and_save_predicted_boxes(condition, epoch, config, limit = None):\n",
    "    \n",
    "    MODEL_DIR = os.path.join(PROJECT_DIR, 'models', '02_train_disc_detection', condition)\n",
    "    CROP_DIR = os.path.join(DATA_DIR, 'test_crops', '03_test_disc_detection', CONDITION)\n",
    "    os.makedirs(CROP_DIR, exist_ok=True)\n",
    "\n",
    "    # Read in metadata\n",
    "    test_df = pd.read_csv(os.path.join(DATA_DIR, 'processed_metadata', condition, 'test.csv'))\n",
    "    \n",
    "    # Create data loder\n",
    "    dataset_test = RSNAMultipleBBoxesDataset(test_df, w = config['box_w'], h_l1_l4 = config['box_h_l1_l4'], h_l5 = config['box_h_l5'])\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=utils.collate_fn,\n",
    "        num_workers=os.cpu_count()\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    trained_model = load_model_disc_detection(state_dict=torch.load(os.path.join(f\"{MODEL_DIR}/epoch_{epoch}/model_dict.pt\"))).to(device)\n",
    "    \n",
    "    trained_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Crop and save images\n",
    "        for j, (images, targets) in enumerate(tqdm.tqdm(test_loader)):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "            predictions = trained_model(images)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                bboxes = get_best_boxes(predictions[i])\n",
    "                save_crop(images[i].cpu(), bboxes, targets[i], crop_dir=CROP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9305d3-0d42-4141-8c2c-345a032425e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spinal Canal Stenosis (captured by Sagittal T2/STIR images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "affab940-76bf-40a4-b3b0-7ea951c9ca1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [01:00<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "CONDITION = 'SpinalCanalStenosis'\n",
    "BEST_EPOCH = 1\n",
    "\n",
    "CONFIG = dict(\n",
    "    num_epochs=3,\n",
    "    batch_size=10,\n",
    "    lr=0.0001,\n",
    "    lr_step_size=3,\n",
    "    lr_gamma=0.1,\n",
    "    box_w = 70, # width of the bounding boxes\n",
    "    box_h_l1_l4 = 30, # height of the boxes for levels from L1/L2 to L4/L5\n",
    "    box_h_l5 = 40 # width of the boxes for level L5/S1\n",
    ")\n",
    "\n",
    "crop_and_save_predicted_boxes(condition = CONDITION, epoch = BEST_EPOCH, config = CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a249a1-6723-4f7a-b1bf-a48b2b2c04fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Left Neural Foraminal Narrowing (captured by Sagittal T1 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8acc018-aa9c-47ce-a2c8-adca0b872601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:36<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "CONDITION = 'LeftNeuralForaminalNarrowing'\n",
    "BEST_EPOCH = 1\n",
    "\n",
    "CONFIG = dict(\n",
    "    num_epochs=3,\n",
    "    batch_size=10,\n",
    "    lr=0.0001,\n",
    "    lr_step_size=3,\n",
    "    lr_gamma=0.1,\n",
    "    box_w = 70, # width of the bounding boxes\n",
    "    box_h_l1_l4 = 30, # height of the boxes for levels from L1/L2 to L4/L5\n",
    "    box_h_l5 = 40 # width of the boxes for level L5/S1\n",
    ")\n",
    "\n",
    "crop_and_save_predicted_boxes(condition = CONDITION, epoch = BEST_EPOCH, config = CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea33fc2-9a4f-4d7a-8a93-dbfa8754e6c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Right Neural Foraminal Narrowing (captured by Sagittal T1 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a810b08e-1a66-45a0-884e-5d1d15c4c641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:34<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "CONDITION = 'RightNeuralForaminalNarrowing'\n",
    "BEST_EPOCH = 1\n",
    "\n",
    "CONFIG = dict(\n",
    "    num_epochs=3,\n",
    "    batch_size=10,\n",
    "    lr=0.0001,\n",
    "    lr_step_size=3,\n",
    "    lr_gamma=0.1,\n",
    "    box_w = 70, # width of the bounding boxes\n",
    "    box_h_l1_l4 = 30, # height of the boxes for levels from L1/L2 to L4/L5\n",
    "    box_h_l5 = 40 # width of the boxes for level L5/S1\n",
    ")\n",
    "\n",
    "crop_and_save_predicted_boxes(condition = CONDITION, epoch = BEST_EPOCH, config = CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d5a4e-82f5-4f32-a1f1-8df28f487435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [base-gpu.py310] (Local)",
   "language": "python",
   "name": "gcr.io_deeplearning-platform-release_base-gpu.py310_latest__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
